---
title: Wallace Session `r Sys.Date()`
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
knit_engines$set(asis = function(options) {
  if (options$echo && options$eval) knit_child(text = options$code)
})
knitr::opts_chunk$set(message = FALSE, warning = FALSE, eval = FALSE)
```

```{r vars, echo = FALSE, include = FALSE, eval = TRUE}
# comp1
occs.db <- rvs$comp1 == 'db'
occs.csv <- rvs$comp1 == 'csv'
occs.any <- occs.db | occs.db
# comp2
poccs.rem <- 'rem' %in% rvs$comp2
poccs.sel <- 'sel' %in% rvs$comp2
poccs.thin <- 'thin' %in% rvs$comp2
poccs.any <- poccs.rem | poccs.sel | poccs.thin
# comp3
env.bc <- rvs$comp3 == 'bc'
env.user <- rvs$comp3 == 'user'
env.any <- env.bc | env.user
# comp4
bg.bb <- rvs$bgSel == 'bb'
bg.mcp <- rvs$bgSel == 'mcp'
bg.ptbuf <- rvs$bgSel == 'ptbuf'
bg.userCSV <- rvs$bgUser == 'csv'
bg.userShp <- rvs$bgUser == 'shp'
bg.bufWid <- rvs$bgBuf > 0
bg.any <- bg.bb | bg.mcp | bg.ptbuf | bg.user
# comp5
part.block <- rvs$partSel == 'block'
part.cb1 <- rvs$partSel == 'cb1'
part.cb2 <- rvs$partSel == 'cb2'
part.jack <- rvs$partSel == 'jack'
part.rand <- rvs$partSel == 'rand'
part.any <- part.block | part.cb1 | part.cb2 | part.jack | part.rand
# comp6
model <- input$goEval != 0
isBC <- if (!is.null(values$evalMods)) input$enmSel == 'BIOCLIM' else FALSE
isMX <- if (!is.null(values$evalMods)) input$enmSel == 'Maxent' else FALSE
# comp7
mapPred <- !is.null(values$goMapPred)
respCurv <- !is.null(values$respCurvParams) & isMX
bcEnvelPlot <- !is.null(values$bcEnvelPlot) & isBC
mxEvalPlot <- !is.null(values$mxEvalPlot) & isMX
isViz <- mapPred | respCurv | bcEnvelPlot | mxEvalPlot
# comp8
projArea <- !is.null(values$projMsk)
projTime <- !is.null(values$projTimeVars)
projAny <- projArea | projTime
mess <- !is.null(values$mess)
```

Please find below the R code history from your *Wallace* session. 

You can reproduce your session results by running this R Markdown file in RStudio. 

Each code block is called a "chunk", and you can run them either one-by-one or all at once by choosing an option in the "Run" menu at the top-right corner of the "Source" pane in RStudio. 

For more detailed information see <http://rmarkdown.rstudio.com>).

### Package installation

Wallace uses the following R packages that must be installed and loaded before starting.
```{r loadPkgs}
library(spocc)
library(spThin)
library(dismo)
library(rgeos)
library(ENMeval)
```

Wallace also includes several functions developed to help integrate different packages and some additional functionality. For this reason, it is necessary to load the file `functions.R`, The function `system.file()` finds this script, and `source()` loads it.

```{r loadFunctions}
source(system.file('shiny/funcs', 'functions.R', package = 'wallace'))
```

## Record of analysis for *`r "{{spName}}"`*.

```{asis, echo = occs.any, eval = occs.any, include = occs.any}
### Obtain Occurrence Data
```

```{asis, echo = occs.db, eval = occs.db, include = occs.db}
The search for occurrences was limited to `r {{occNum}}` records. Obtain occurrence records of the selected species from the `r "{{dbName}}"` database.
```

```{r occSearch, echo = occs.db, include = occs.db}
# query selected database for occurrence records
results <- spocc::occ(query = "{{spName}}", from = "{{dbName}}", limit = {{occNum}}, has_coords = TRUE)
# retrieve data table from spocc object
results.data <- results[["{{dbName}}"]]$data[[formatSpName("{{spName}}")]]
# remove rows with duplicate coordinates
occs <- remDups(results.data)
# make sure latitude and longitude are numeric (sometimes they are characters)
occs$latitude <- as.numeric(occs$latitude)
occs$longitude <- as.numeric(occs$longitude)
# give all records a unique ID
occs$occID <- row.names(occs)
```

```{asis, echo = occs.csv, eval = occs.csv, include = occs.csv}
User CSV path with occurrence data. If the CSV file is not in the current workspace, change to the correct file path (e.g. "/Users/darwin/Documents/occs.csv").
```

```{r occInput, echo = occs.csv}
inFile <- read.csv("{{occsCSV}}", header = TRUE)  # load occurrence data
occs <- remDups(inFile)  # remove duplicate records
```




```{asis, echo = poccs.any, eval = poccs.any, include = poccs.any}
### Process Occurrence Data
```

```{asis, echo = poccs.rem, eval = poccs.rem, include = poccs.rem}
Remove the occurrence localities with the following IDs: `r {{occsRemoved}}`.
```

```{r poccs.rem, echo = poccs.rem, include = poccs.rem}
remo <- which(occs$occID %in% {{occsRemoved}})  # find row numbers that match IDs to be removed
occs <- occs[-remo, ]  # remove the selected rows
```

```{asis, echo = poccs.sel, eval = poccs.sel, include = poccs.sel}
You chose `r length({{occsSel}})` total occurrence localities via polygon selection to keep in the analysis.
```

```{r remLocs2, echo = poccs.sel, include = poccs.sel}
occs <- occs[{{occsSel}}, ]  # subset occs by selected rows
```

```{asis, echo = poccs.thin, eval = poccs.thin, include = poccs.thin}
Spatial thinning selected. Thin distance selected is `r {{thinDist}}` m.
```

```{r doThin, echo = poccs.thin, include = poccs.thin}
output <- spThin::thin(occs, 'latitude', 'longitude', 'name', thin.par = {{thinDist}}, reps = 100, locs.thinned.list.return = TRUE, write.files = FALSE, verbose = FALSE)
```

```{asis, echo = poccs.thin, eval = poccs.thin, include = poccs.thin}
Since spThin did 100 iterations, there are 100 different variations of how it thinned your occurrence localities. As there is a stochastic element in the algorithm, some iterations may include more localities than the others, and we need to make sure we maximize the number of localities we proceed with.
```

```{r doThin2, echo = poccs.thin, include = poccs.thin}
maxThin <- which(sapply(output, nrow) == max(sapply(output, nrow)))  # find the iteration that returns the max number of occurrences
maxThin <- output[[ifelse(length(maxThin) > 1, maxThin[1], maxThin)]]  # if there's more than one max, pick the first one
occs <- occs[as.numeric(rownames(maxThin)),]  # subset occs to match only thinned occs
```




```{asis, echo = env.any, eval = env.any, include = env.any}
### Obtain Environmental Data
```

``` {asis, echo = env.bc, eval = env.bc, include = env.bc}
Using WorldClim (http://www.worldclim.org/) bioclimatic dataset at resolution of `r {{bcRes}}` arcmin.
```

```{r getEnvBC, echo = env.bc, include = env.bc}
# get WorldClim bioclimatic variable rasters
envs <- raster::getData(name = "worldclim", var = "bio", res = {{bcRes}}, lat = {{bcLat}}, lon = {{bcLon}})
# extract environmental values at occ grid cells
locs.vals <- raster::extract(envs[[1]], occs[, c('longitude', 'latitude')])
# remove occs without environmental values
occs <- occs[!is.na(locs.vals), ]  
```

```{r getEnvUser, echo = env.user, include = env.user}
envs <- raster::stack("{{userEnvsPath}}")
```




```{r switch, include = FALSE, eval = backg}
bgType <- switch("{{bgSel}}", 'bb'='Bounding Box', 'mcp'='Minimum Convex Polygon',   
                'ptsbuf'='Buffered Points',  'user'='User-defined')
```

```{asis, echo = bg.any, eval = bg.any, include = bg.any}
### Process Environmental Data
Background selection technique chosen as `r bgType`.
```

```{r bgBB, echo = bg.bb, include = bg.bb}
xmin <- min(occs$longitude)
xmax <- max(occs$longitude)
ymin <- min(occs$latitude)
ymax <- max(occs$latitude)
bb <- matrix(c(xmin, xmin, xmax, xmax, xmin, ymin, ymax, ymax, ymin, ymin), ncol=2)
backgExt <- sp::SpatialPolygons(list(sp::Polygons(list(sp::Polygon(bb)), 1)))
```

```{r bgMCP, echo = bg.mcp, include = bg.mcp}
backgExt <- mcp(occs[,2:3])
```

```{asis, echo = bg.userCSV, eval = bg.userCSV, include = bg.userCSV}
User study extent name is `r "{{bgUserCSVname}}"`. User study extent path is `r "{{bgUserCSVpath}}"`. Read in the .csv file and generate a Spatial Polygon object.
```

```{r bgUserCSV, echo = bg.userCSV, include = bg.userCSV}
# read csv with coordinates for polygon
shp <- read.csv("{{bgUserCSVpath}}", header = TRUE)
backgExt <- sp::SpatialPolygons(list(sp::Polygons(list(sp::Polygon(shp)), 1)))
```

```{asis, echo = bg.userShp, eval = bg.userShp, include = bg.userShp}
User study extent name is `r "{{bgUserShpName}}"`. User study extent path is `r "{{bgUserShpPath}}"`. Read in the .csv file and generate a Spatial Polygon object.
```

```{r bgUserShp, echo = bg.userCSV, include = bg.userCSV}
# read csv with coordinates for polygon
backgExt <- rgdal::readOGR("{{bgUserShpPath}}", "{{bgUserShpName}}")
```

```{asis, echo = bg.bufWid, eval = bg.bufWid, include = bg.bufWid}
Buffer size of the study extent polygon defined as `r {{bgBuf}}` km.
```

```{r bgBufWid, echo = bg.bufWid, include = bg.bufWid}
backgExt <- rgeos::gBuffer(backgExt, width = {{bgBuf}})
```

```{asis, echo = bg.any, eval = bg.any, include = bg.any}
Mask environmental variables by `r bgType`, and take a random sample of background values from the study extent. As the sample is random, your results may be different than those in the session. If there seems to be too much variability in these background samples, try increasing the number from 10,000 to something higher (e.g. 50,000 or 100,000). The better your background sample, the less variability you'll have between runs.
```

```{r envsMask, echo = bg.any, include = bg.any}
envsBackgCrop <- raster::crop(envs, backgExt)
envsBackgMsk <- raster::mask(envsBackgCrop, backgExt)
occs.xy <- occs[,2:3]
bg.xy <- dismo::randomPoints(envsBackgMsk, {{bgPtsNum}})  # generate 10,000 background points
bg.xy <- as.data.frame(bg.xy)  # get the matrix output into a data.frame
```




```{asis, echo = part, eval = part, include = part}
### Partition Occurrence Data
Here, occurrence data is partitioned for cross-validation.
```

```{r switch2, include = FALSE, eval = part}
partSwitch <- switch("{{partSel}}", 'block'='Block', 'cb1'='Checkerboard 1', 'cb2'='Checkerboard 2', 'jack'='Jackknife', 'random'='Random')
```

```{asis, echo = part, eval = part, include = part}
Data partition by `r partSwitch` method.
```

```{r blockGrp, echo = isBlock, include = isBlock}
group.data <- ENMeval::get.block(occ=occs.xy, bg.xy=bg.xy)
```

```{r check1Grp, echo = isCB1, include = isCB1}
group.data <- ENMeval::get.checkerboard1(occ=occs.xy, env=envsBackgMsk, bg.xy=bg.xy, aggregation.factor={{aggFact}})
```

```{r check2Grp, echo = isCB2, include = isCB2}
group.data <- ENMeval::get.checkerboard2(occ=occs.xy, env=envsBackgMsk, bg.xy=bg.xy, aggregation.factor={{aggFact}})
```

```{r jackGrp, echo = isJack, include = isJack}
group.data <- ENMeval::get.jackknife(occ=occs.xy, bg.xy=bg.xy)
```

```{r randGrp, echo = isRandom, include = isRandom}
group.data <- ENMeval::get.randomkfold(occ=occs.xy, bg.xy=bg.xy, kfolds={{kfolds}})
```

```{asis, echo = part, include = part}
Define modeling parameters:
```

```{r params, echo = part, include = part}
modParams <- list(occ.pts=occs.xy, bg.pts=bg.xy, occ.grp=group.data[[1]], bg.grp=group.data[[2]])
```


```{asis, echo = model, eval = model, include = model}
### Build and Evaluate Niche Model
`r "{{enmSel}}"` model selected.
```

```{r bioclim, echo = isBC, include = isBC}
e <- BioClim_eval(modParams$occ.pts, modParams$bg.pts, modParams$occ.grp, modParams$bg.grp, envsBackgMsk)
evalTbl <- e$results
evalMods <- e$models
names(e$predictions) <- "Classic_BIOCLIM"
evalPreds <- e$predictions
occVals <- raster::extract(e$predictions, modParams$occ.pts)  # get predicted values for occ grid cells
mtps <- min(occVals)  # apply minimum training envsence threshold
# Define 10% training envsence threshold
if (length(occVals) < 10) {  # if less than 10 occ values, find 90% of total and round down
  n90 <- floor(length(occVals) * 0.9)
} else {  # if greater than or equal to 10 occ values, round up
  n90 <- ceiling(length(occVals) * 0.9)
}
p10s <- rev(sort(occVals))[n90]  # apply 10% training presence threshold
```

```{r maxent, echo = isMX, results = 'hide'}
rms <- seq({{rmsSel1}}, {{rmsSel2}}, {{rmsBy}})  # define the vector of RMs to input
e <- ENMeval::ENMevaluate(modParams$occ.pts, predsBackgMsk, bg.xy=modParams$bg.pts, RMvalues=rms, fc={{fcsSel}}, method='user', occ.grp=modParams$occ.grp, bg.grp=modParams$bg.grp)
evalTbl <- e@results
evalMods <- e@models
evalPreds <- e@predictions  # raw predictions

evalPredsLog <- stack(sapply(e@models, function(x) predict(x, predsBackgMsk)))  # logistic predictions

occVals <- raster::extract(e@predictions, modParams$occ.pts)  # get predicted values for occ grid cells
mtps <- apply(occVals, MARGIN = 2, min)  # apply minimum training presence threshold over all models
# Define 10% training presence threshold
if (length(occVals) < 10) {  # if less than 10 occ values, find 90% of total and round down
  n90 <- floor(length(occVals) * 0.9)
} else {  # if greater than or equal to 10 occ values, round up
  n90 <- ceiling(length(occVals) * 0.9)
}
p10s <- apply(occVals, MARGIN = 2, function(x) rev(sort(x))[n90])  # apply 10% training presence threshold over all models
```

```{asis, echo = isViz, eval = isViz, include = isViz}
### Visualize Niche Model
You chose to view visualizations of your modeling analysis results.
```

```{r respCurv, echo = respCurv, include = respCurv}
dismo::response(evalMods[["{{modSel}}"]], var = "{{envSel}}")
```

```{r bcPlot, echo = bcEnvelPlot, include = bcEnvelPlot}
plot(evalMods[[1]], a = {{bcPlot1}}, b = {{bcPlot2}}, p = {{bcPlotP}})
```

```{r evalPlot, echo = mxEvalPlot, include = mxEvalPlot}
ENMeval::eval.plot(evalTbl, value = "{{mxEvalPlotSel}}")
```

```{asis, echo = projAny, eval = projAny, include = projAny}
### Project Niche Model
You selected to project your model. First define a polygon with the coordinates you chose, then crop and mask your predictor rasters. Finally, predict suitability values for these new raster cells based on the model you selected.
```

```{r projSel, echo = projAny, include = projAny}
projCoords <- data.frame(x = {{projAreaX}}, y = {{projAreaY}})
projPoly <- sp::SpatialPolygons(list(sp::Polygons(list(sp::Polygon(projCoords)), ID=1)))

# Select your model from the evalMods list
modSel <- evalMods[[{{modSel}}]]
```

```{asis, echo = projArea, eval = projArea, include = projArea}
### Project Niche Model to New Extent
Now use crop and mask the predictor variables by projPoly, and use the predict() function to predict the values for the new extent based on the model selected.
```

```{r projArea, echo = projArea, include = projArea}
predsProj <- raster::crop(envs, projPoly)
predsMsk <- raster::mask(envsProj, projPoly)

newExtProj <- dismo::predict(modSel, envsMsk)
plot(newExtProj)
```

```{asis, echo = projTime, eval = projTime, include = projTime}
### Project Niche Model to New Time
Now download the future climate variables chosen with *Wallace*, crop and mask them by projPoly, and use the predict() function to predict the values for the new time based on the model selected.
```

```{r projTime, echo = projTime, include = projTime}
envsFuture <- raster::getData("CMIP5", var = "bio", res = {{bcRes}}, rcp = {{selRCP}}, model = "{{selGCM}}", year = {{selTime}})

envsProj <- raster::crop(envsFuture, projPoly)
envsMsk <- raster::mask(envsProj, projPoly)

# rename future climate variable names
names(envsMsk) <- names(envs)
futureProj <- dismo::predict(modSel, envsMsk)
plot(futureProj)
```

```{asis, echo = mess, eval = mess, include = mess}
### Calculate Environmental Similarity
To visualize the environmental difference between the occurrence localities and your selected projection extent, calculate a multidimensional environmental similarity surface (MESS). High negative values mean great difference, whereas high positive values mean great similarity. Interpreting the projected suitability for areas with high negative values should be done with extreme caution, as they are outside the environmental range of the occurrence localities.
```

```{r projMESS, echo = mess, include = mess}
# extract environmental values from occurrence localities -- these were the values that went into the model
occVals <- raster::extract(envs, occs.xy)
# compare these values with the projection extent (envsMsk)
occVals.mess <- dismo::mess(envsMsk, occVals)
plot(occVals.mess)
```
