---
title: Wallace Session `r Sys.Date()`
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
knit_engines$set(asis = function(options) {
  if (options$echo && options$eval) knit_child(text = options$code)
})
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

This is an R Markdown document (for more information see <http://rmarkdown.rstudio.com>). Here all R code history from the Wallace session is recorded and annotated. With this document, users can track the analyses completed in their session and reproduce them by running this file in RStudio.

###Package installation

Wallace uses the following R packages that must be installed and loaded before starting.
```{r}
library(devtools)
library(spocc)
library(maptools)
library(spThin)
library(dismo)
library(rgeos)
library(repmis)
library(maps)
library(ENMeval)
```

Wallace also includes several functions developed to help integrate different packages and some additional functionality. For this reason, it is necessary to load the file 'functions.R', which can be found on Wallace's GitHub page (https://github.com/ndimhypervol/wallace). Download the file, place it in your working directory (use `getwd()`), and then load it:
```{r loadFunctions}
source(file.path("{{curWD}}", 'functions.R'))
```

```{r, echo = FALSE, include = FALSE}
occ <- values$mod_db
csv <- !is.null(input$userCSV)
occ.csv <- occ | csv
remLoc <- !is.null(values$removedAll)
drawPoly <- !is.null(values$ptSel)
thin <- input$goThin != 0
yesPreds <- input$bcRes != ""
backg <- !is.null(values$backgExt)
isBB <- if (backg) input$backgSel == 'bb' else FALSE
isMCP <- if (backg) input$backgSel == 'mcp' else FALSE
isUserBG <- if (backg) input$backgSel == 'user' else FALSE
part <- input$goPart != 0
isBlock <- if (!is.null(values$partSel2)) values$partSel2 == 'block' else FALSE
isCB1 <- if (!is.null(values$partSel2)) values$partSel2 == 'cb1' else FALSE
isCB2 <- if (!is.null(values$partSel2)) values$partSel2 == 'cb2' else FALSE
isJack <- if (!is.null(values$partSel2)) values$partSel2 == 'jack' else FALSE
isRandom <- if (!is.null(values$partSel2)) values$partSel2 == 'random' else FALSE
model <- input$goEval != 0
isBC <- if (!is.null(values$evalMods)) input$enmSel == 'BIOCLIM' else FALSE
isMX <- if (!is.null(values$evalMods)) input$enmSel == 'Maxent' else FALSE
```

Record of analysis for *`r values$spName`*.

```{asis, echo = occ.csv, include = occ.csv}
### Obtain Occurrence Data
```

```{asis, echo = occ, include = occ}
The search for occurrences was limited to `r {{occNum}}` records. Obtain occurrence records of the selected species from the database:
```

```{r occSearch, eval = occ, echo = occ, include = occ}
results <- occ(query = "{{spName}}", from = "{{dbName}}", limit = {{occNum}}, has_coords = TRUE)
results.data <- results[["{{dbName}}"]]$data[[formatSpName("{{spName}}")]]
occs <- remDups(results.data)  # remove rows with duplicate coordinates
```

```{asis, echo = csv}
User CSV path with occurrence data (change to the path of the file in your computer):
```

```{r input, echo = csv, eval = csv}
inFile <- read.csv("{{occsCSV}}", header = TRUE)  # load occurrence data
occs <- remDups(inFile)  # remove duplicate records
```

```{asis, echo = thin | remLoc | drawPoly, include = drawPoly}
### Process Occurrence Data
```

```{asis, echo = remLoc, eval = remLoc, include = remLoc}
Remove the following occurrence localities: `r {{occsRemoved}}`.
```

```{r, echo = remLoc, eval = remLoc, include = remLoc}
rows <- as.numeric(rownames(occs))  # get row names
remo <- which({{occsRemoved}} %in% rows)  # find which row names correspond to user selection for removal
occs <- occs[-remo, ]  # remove the selected rows
```

```{asis, echo = drawPoly, eval = drawPoly, include = drawPoly}
You chose `r length({{occsSel}})` of `r nrow(occs)` total occurrence localities via polygon selection to keep in the analysis.
```

```{r, echo = drawPoly, eval = drawPoly, include = drawPoly}
occs <- occs[{{occsSel}}, ]  # subset occs by selected rows
```

```{asis, echo = thin, include = thin}
Spatial thinning selected. Thin distance selected is `r {{thinDist}}` m.
```

```{r doThin, echo = thin, eval = thin, include = thin}
output <- thin(occs, 'latitude', 'longitude', 'species', thin.par = {{thinDist}}, reps = 100, locs.thinned.list.return = TRUE, write.files = FALSE, verbose = FALSE)
```

```{asis, echo = thin, include = thin}
Since spThin did 100 iterations, there are 100 different variations of how it thinned your occurrence localities. As there is a stochastic element in the algorithm, some iterations may include more localities than the others, and we need to make sure we maximize the number of localities we proceed with.
```

```{r, echo = thin, eval = thin, include = thin}
maxThin <- which(sapply(output, nrow) == max(sapply(output, nrow)))  # find the iteration that returns the max number of occurrences
maxThin <- output[[ifelse(length(maxThin) > 1, maxThin[1], maxThin)]]  # if there's more than one max, pick the first one
occs <- occs[as.numeric(rownames(maxThin)),]  # subset occs to match only thinned occs
```


```{asis, echo = yesPreds, eval = yesPreds, include = yesPreds}
### Obtain Environmental Data
Using WorldClim (http://www.worldclim.org/) bioclim dataset at resolution of `r {{predsRes}}` arcmin.
```

```{r getPreds, eval = yesPreds, echo = yesPreds, include = yesPreds}
preds <- getData(name = "worldclim", var = "bio", res = {{predsRes}}, lat = {{bcLat}}, lon = {{bcLon}})
locs.vals <- extract(preds[[1]], occs[, c('longitude', 'latitude')])  # extract environmental values at occ grid cells
occs <- occs[!is.na(locs.vals), ]  # remove occs without environmental values from inFile
```


```{r, include = FALSE, eval = backg}
backgSwitch <- switch("{{backgSel}}", 'bb'='Bounding Box', 'mcp'='Minimum Convex Polygon', 'user'='User-defined')
```

```{asis, echo = backg, eval = backg, include = backg}
### Process Environmental Data
Background selection technique chosen as `r backgSwitch`.
Buffer size of the study extent polygon defined as `r {{backgBuf}}` km.
```

```{r, echo = isBB, eval = isBB, include = isBB}
xmin <- min(occs$longitude) - ({{backgBuf}} + res(preds)[1])
xmax <- max(occs$longitude) + ({{backgBuf}} + res(preds)[1])
ymin <- min(occs$latitude) - ({{backgBuf}} + res(preds)[1])
ymax <- max(occs$latitude) + ({{backgBuf}} + res(preds)[1])
bb <- matrix(c(xmin, xmin, xmax, xmax, xmin, ymin, ymax, ymax, ymin, ymin), ncol=2)
backgExt <- SpatialPolygons(list(Polygons(list(Polygon(bb)), 1)))
```

```{r, echo = isMCP, eval = isMCP, include = isMCP}
xy_mcp <- mcp(occs[,2:3])
bb <- xy_mcp@polygons[[1]]@Polygons[[1]]@coords
backgExt <- gBuffer(xy_mcp, width = {{backgBuf}} + res(preds)[1])
```

```{asis, echo = isUserBG, eval = isUserBG, include = isUserBG}
User study extent name is `r "{{userBGname}}"`. User study extent path is `r "{{userBGpath}}"`.
```

```{r, echo = isUserBG, eval = isUserBG, include = isUserBG}
shp <- read.csv("{{userBGpath}}", header = TRUE)  # read csv with coordinates for polygon
```

```{asis, echo = backg, include = backg}
Generate the user-defined study extent plus the buffer:
```

```{r, echo = isUserBG, eval = isUserBG, include = isUserBG}
shp <- SpatialPolygons(list(Polygons(list(Polygon(shp)), 1)))
shp <- gBuffer(shp, width = {{backgBuf}} + res(preds)[1])
backgExt <- shp
bb <- shp@polygons[[1]]@Polygons[[1]]@coords
```

```{asis, echo = backg, include = backg}
Mask environmental variables by `r backgSwitch`:
```

```{r, echo = backg, eval = backg, include = backg}
predCrop <- crop(preds, backgExt)
predsMsk <- mask(predCrop, backgExt)
```


```{asis, echo = part, include = part}
### Partition Occurrence Data
Here, occurrence data is partitioned for cross-validation.
```

```{r, echo = part, eval = part, include = part}
occs.locs <- occs[,2:3]
bg.coords <- randomPoints(predsMsk, 10000)  # generate 10,000 background points
bg.coords <- as.data.frame(bg.coords)  # get the matrix output into a data.frame
```

```{r, include = FALSE, eval = part}
partSwitch <- switch("{{partSel}}", 'block'='Block', 'cb1'='Checkerboard 1', 'cb2'='Checkerboard 2', 'jack'='Jackknife', 'random'='Random')
```

```{asis, echo = part, eval = part, include = part}
Data partition by `r partSwitch` method.
```

```{r, echo = isBlock, eval = isBlock, include = isBlock}
group.data <- get.block(occ=occs.locs, bg.coords=bg.coords)
```

```{r, echo = isCB1, eval = isCB1, include = isCB1}
group.data <- get.checkerboard1(occ=occs.locs, env=predsMsk, bg.coords=bg.coords, aggregation.factor={{aggFact}})
```

```{r, echo = isCB2, eval = isCB2, include = isCB2}
group.data <- get.checkerboard2(occ=occs.locs, env=predsMsk, bg.coords=bg.coords, aggregation.factor={{aggFact}})
```

```{r, echo = isJack, eval = isJack, include = isJack}
group.data <- get.jackknife(occ=occs.locs, bg.coords=bg.coords)
```

```{r, echo = isRandom, eval = isRandom, include = isRandom}
group.data <- get.randomkfold(occ=occs.locs, bg.coords=bg.coords, kfolds={{kfoldsSel}})
```

```{asis, echo = part, include = part}
Define modeling parameters:
```

```{r, echo = part, eval = part, include = part}
modParams <- list(occ.pts=occs.locs, bg.pts=bg.coords, occ.grp=group.data[[1]], bg.grp=group.data[[2]])
```


```{asis, echo = model, eval = model, include = model}
### Build and Evaluate Niche Model
`r "{{enmSel}}"` model selected.
```

```{r, echo = isBC, eval = isBC, include = isBC}
e <- BioClim_eval(modParams$occ.pts, modParams$bg.pts, modParams$occ.grp, modParams$bg.grp, predsMsk)
evalTbl <- e$results
evalMods <- e$models
names(e$predictions) <- "Classic_BIOCLIM"
evalPreds <- e$predictions
occVals <- extract(e$predictions, modParams$occ.pts)  # get predicted values for occ grid cells
mtps <- min(occVals)  # apply minimum training presence threshold
# Define 10% training presence threshold
if (length(occVals) < 10) {  # if less than 10 occ values, find 90% of total and round down
  n90 <- floor(length(occVals) * 0.9)
} else {  # if greater than or equal to 10 occ values, round up
  n90 <- ceiling(length(occVals) * 0.9)
}
p10s <- rev(sort(occVals))[n90]  # apply 10% training presence threshold
```

```{r, echo = isMX, eval = isMX, results = 'hide'}
rms <- seq({{rmsSel1}}, {{rmsSel2}}, {{rmsBy}})  # define the vector of RMs to input
e <- ENMevaluate(modParams$occ.pts, predsMsk, bg.coords=modParams$bg.pts, RMvalues=rms, fc={{fcsSel}}, method='user', occ.grp=modParams$occ.grp, bg.grp=modParams$bg.grp)
evalTbl <- e@results
evalMods <- e@models
evalPreds <- e@predictions.raw
evalPredsLog <- e@predictions.log
occVals <- extract(e@predictions.raw, modParams$occ.pts)  # get predicted values for occ grid cells
mtps <- apply(occVals, MARGIN = 2, min)  # apply minimum training presence threshold over all models
# Define 10% training presence threshold
if (length(occVals) < 10) {  # if less than 10 occ values, find 90% of total and round down
  n90 <- floor(length(occVals) * 0.9)
} else {  # if greater than or equal to 10 occ values, round up
  n90 <- ceiling(length(occVals) * 0.9)
}
p10s <- apply(occVals, MARGIN = 2, function(x) rev(sort(x))[n90])  # apply 10% training presence threshold over all models
```
